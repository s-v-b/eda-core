---
title: 'LAB: Linear Regression on Whiteside data'
date: "`r Sys.time()`"
draft: false 

execute:
  echo: true
  eval: true
  collapse: true


format:
  html:
    output-file: lab-linear-regression-whiteside.html
  pdf:
    output-file: lab-linear-regression-whiteside.pdf


---


{{< include _preamble.qmd >}}



## Introduction

The purpose of this lab is to introduce *linear regression* using base `R` and the tidyverse. 
We work on a dataset provided by the [MASS](https://www.stats.ox.ac.uk/pub/MASS4/) package.
This dataset is investigated in the book by Venables and Ripley. This discusssion is  worth being read. Our aim is to relate regression as a tool for data exploration with regression as a method in statistical inference. To perform regression, we will rely on the base `R` function  `lm()` and on the eponymous S3 class `lm`. We will spend time understanding how the *formula* argument can be used to  construct a *design matrix* from a dataframe representing a dataset.  



## Packages installation and loading  (again)



```{r}
#| label: setup-packages
#| echo: true
#| message: false
#| warning: false
#| include: true

# We will use the following packages. 
# If needed, install them : pak::pkg_install(). 
stopifnot(
  require("magrittr"),
  require("lobstr"),
  require("ggforce"),
  require("patchwork"), 
  require("gt"),
  require("glue"),
  require("skimr"),
  require("corrr"),
  require("GGally"),
  require("broom"),
  require("tidyverse"),
  require("ggfortify"),
  require("autoplotly")

)
```

Besides the `tidyverse`, we rely on `skimr` to perform univariate analysis, `GGally::ggpairs` to perform pairwise (bivariate) analysis. Package `corrr` provide graphical tools to explore correlation matrices.
At some point, we will showcase the exposing pipe `%$%` and the classical pipe `%>%` of `magrittr`. We use `gt` to display handy tables, `patchwork` to compose graphical objects. `glue` provides a kind of formatted strings. Package `broom` proves very useful when milking lienar models produced by `lm()` (and many other objects produced by estimators, tests, ...)   


```{r}
#| echo: false
#| include: false
#| eval: true
#| label: theming
#| 
# Setting the look and feel of our plots
old_theme <- theme_set(
  theme_minimal()
)
```


# Dataset

The dataset is available from package `MASS`. `MASS` can be downloaded from `cran`.


```{r}
#| eval: true
#| label: whiteside-load
whiteside <- MASS::whiteside # no need to load the whole package

cur_dataset <- str_to_title(as.character(substitute(whiteside)))
# ?whiteside
```

The documentation of `R` tells us a little bit more about this data set. 

> Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.

This means that our sample is  made of 56 observations. Each observation corresponds to a week during heating season. For each observation. We have the average external temperature `Temp` (in degrees Celsius) and the weekly gas consumption `Gas`. We also have `Insul` which tells us whether the observation has been recorded `Before` or `After` treatment.  

Temperature is the *explanatory* variable or the *covariate*. The target/response is the weekly Gas Consumption. We aim to *predict* or to *explain* the variations of weekly gas consumption as a function average weekly temperature. 

The question is wether  the  treatment (insulation) modifies the relation between gas consumption and external temperature, and if we conclude that the treatment modifies the relation, in which way?.

Have a glimpse at the data. 

```{r}
#| label: whiteside-glimpse
whiteside %>% 
  glimpse
```

Even though the experimenter, Mr Whiteside, decided to apply a *treatment* to his house. This is not exactly what we call *experimental data*.  Namely, the experimenter has no way to clamp the external temperature. 
With respect to the Temperature variable (the explanatory variable) we are facing *observational* data.




# Columnwise exploration


::: {.callout-note title="Question"}

Before before proceeding to linear regressions of `Gas` with respect to `Temp`, perform univariate analysis on each variable.  

- Compute summary statistics
- Build the corresponding plots
  
:::

::: {.content-visible when-profile='solution'} 

::: {.callout-tip title="Solution"}


`skimr` does the job.  There are no missing data, complete rate is always `1`, we remove non-informative 
columns from the output. 

```{r}
#| label: skim-it
whiteside |>
  skimr::skim() |>
  select(-n_missing, -complete_rate, -factor.ordered, - factor.n_unique) |>
  gt() |>
  gt::fmt_number(decimals=1) |>
  gt::tab_caption("Whiteside dataset. Columnwise summaries")
```

An  alternative way of doing this univariate  analysis consists in separating categorical variables from numerical ones. 

```{r}
#| label: catego-summaries
#| eval: false
#| echo: true
sk <- whiteside %>% 
  skimr::skim() %>% 
  select(-n_missing, - complete_rate, -factor.ordered, - factor.n_unique)

skimr::yank(sk, "factor") |> 
  gt() |>
  gt::tab_caption("Whiteside dataset. Categorical variables summaries")


skimr::yank(sk, "numeric") |> 
  gt() |>
  gt::fmt_number(decimals=1) |>
  gt::tab_caption("Whiteside dataset. Categorical variables summaries")
```

We may test the normality of the `Gas`  and `Temp`  distribution

```{r}
#| appendix: true
#| label: shapiro-test
#| echo: false
test_norm_num <- function(df, ze_test=shapiro.test) {
  summarise(df,
            across(where(is.numeric),
                    ~ list(ze_test(.x)|> tidy())
            )
    ) |>
  pivot_longer(cols=everything(),
               names_to = "Var",
               values_to="test") |>
  unnest(cols=c(test)) |>
  gt() |> 
  fmt_number(decimals=2) 
}

whiteside |> 
  test_norm_num()
```

Both variables pass the Shapiro-Wilk test with flying colors. 

We may use the Jarque-Bera test [See R bloggers on this](https://www.r-bloggers.com/2021/08/goodness-of-fit-test-jarque-bera-test-in-r/)

```{r}
#| label: jb-test
#| appendix: true
#| echo: false
#| warning: false
#| message: false
jb <- tseries::jarque.bera.test

whiteside |> 
  test_norm_num(ze_tes=jb) |>
  tab_caption("Whiteside dataset")
```

Both variables also pass the Jarque-Bera test with flying colors. This is noteworthy 
since the Jarque-Bera compares empirical skewness and kurtosis to Gaussian skewness and kurtosis. 


:::

:::

# Pairwise exploration

::: {.callout-note title="Question"}

Compare distributions of numeric variables with respect to categorical variable `Insul`

:::


::: {.content-visible when-profile='solution'} 
 
::: {.callout-tip title="Solution"}

We start by plotting histograms 

To abide to the DRY principle,  we take 
advantage of the fact that aesthetics and labels can be tuned incrementally. 

```{r}
#| label: histo-seasons
p_xx <- whiteside |>
    ggplot() +
      geom_histogram(
        mapping=aes(fill=Insul, color=Insul, y=after_stat(density)),
        position="dodge",
        alpha=.1,
        bins=10) 

p_1 <- p_xx + 
  aes(x=Temp)  +
  theme(legend.position = "None")+
  labs(title="External Temperature",
       subtitle = "Weekly Average (Celsius)")

p_2 <- p_xx + 
  aes(x=Gas)  +
  labs(title="Gas Consumption" ,
       subtitle="Weekly Average")

# patchwork the two graphical objects 
(p_1 + p_2) +
  patchwork::plot_annotation(
    caption="Whiteside dataset from package MASS"
  )
```

Note the `position` parameter for `geom_histogram()`. Check the result for `position="stack"`, `position="identity"`. Make up your mind about the most convenient choice.

Gas Consumption distribution `After` looks shifted with respect to distribution `Before`.

Another visualization strategy consists in using the faceting mechanism

```{r}
#| label: before-after-histo-facet
r <- whiteside %>% 
  pivot_longer(cols=c(Gas, Temp),
              names_to = "Vars",
              values_to = "Vals") %>% 
  ggplot() +
  aes(x=Vals)  +
  facet_wrap(~ Insul + Vars ) + 
  xlab("")

r +
  aes(y=after_stat(density)) +
  geom_histogram(alpha=.3, fill="white", color="black", bins=6) +
  ggtitle(glue("{cur_dataset} data"))
```


```{r}
#| label: anova-temp-insul-1

lm(Temp ~ Insul, data=whiteside) |>
  broom::glance()
```

```{r}
#| label: anova-temp-insul-2

lm(Temp ~ Insul, data=whiteside) |>
  anova()  |>
  broom::tidy() |>
  gt::gt() |>
  gt::fmt_number(decimals = 3) |>
  gt::tab_caption("Testing temperature homogeneity over the seasons")
```

The difference between temperature distributions over the two seasons is not deemed to be significant by ANOVA. 

It is important that the grouping variable (here `Insul`) is a factor.


:::

:::


# Covariance and correlation between `Gas` and `Temp`


::: {.callout-note title="Question"}

Compute the covariance matrix of `Gas` and `Temp`

:::

::::: {.content-visible when-profile="solution"}  

::: {.callout-tip title="Solution"}


```{r}
#| label: using-cov
# Compute the covariance matrix for Gas and Temp
C <- whiteside |>
  select(where(is.numeric)) |> 
  cov()

mu_n <- whiteside %>% 
  select(where(is.numeric)) %>% 
  colMeans()
```



```{r}
#| output: asis
#| caption: "Covariance matrix of Gas and Temp"
matador::mat2latex(round(C,2))
```


:::

:::::

::: {.callout-note title="Question"}



- Compute correlations (Pearson, Kendall, Spearman) and correlations per group
- Comment

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


```{r}
# use magrittr pipe %>%  to define a pipeline function
my_cor <-  . %>% 
  summarize(
    pearson= cor(Temp, Gas, method="pearson"),
    kendall=cor(Temp, Gas, method="kendall"),
    spearman=cor(Temp,Gas, method="spearman")
  ) 

# even better: pass column names as arguments

my_cor2 <-  function(df, col1, col2){
  df %>% 
  summarize(
    pearson= cor({{col1}}, {{col2}}, method="pearson"),
    kendall=cor({{col1}}, {{col2}}, method="kendall"),
    spearman=cor({{col1}}, {{col2}}, method="spearman")
  ) 
}

t1 <- whiteside |>
  group_by(Insul) |> 
  my_cor()

# group_by(whiteside, Insul) |> my_cor2(Gas, Temp)

t2 <- whiteside |>
  my_cor() |>
  mutate(Insul="pooled")

# whiteside |> my_cor2(Gas, Temp) |> mutate(Insul="pooled")

bind_rows(t1, t2)  |> 
    gt() |>
    gt::fmt_number(decimals=2) |>
    gt::tab_caption("Whiteside data: correlations between Gas and Temp")
```

Note the sharp increase of all correlation coefficients we data are grouped according to the control/treatment variable (`Insul`).

:::

:::

::: {.callout-note title="Question"}


Use `ggpairs` from `GGally` to get a quick overview of the pairwise interactions.

:::

::::: {.content-visible when-profile="solution"}  

::: {.callout-tip title="Solution"}

```{r}
#| label: ggally-ggpairs
#| warning: false
#| message: false


theme_set(theme_minimal())

whiteside |>
  GGally::ggpairs()
```

:::

:::::






::: {.callout-note title="Question"}

Build a scatterplot of the Whiteside dataset


:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: whiteside-scatter
p <- whiteside |> 
  ggplot() +
  aes(x=Temp, y=Gas) +
  geom_point(aes(shape=Insul)) +
  xlab("Average Weekly Temperature (Celsius)") +
  ylab("Average Weekly Gas Consumption 1000 cube feet") +
  labs(
    ## Use list unpacking
  )

p + 
  ggtitle(glue("{cur_dataset} dataset"))

```

Note that the dataset looks like the stacking of two bananas corresponding to the two heating seasons. 
:::

:::

::: {.callout-note title="Question"} 

Build boxplots of `Temp` and  `Gas` versus `Insul`

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: boxplots
q <- whiteside %>% 
  ggplot() +
  aes(x=Insul)

qt <- q + 
  geom_boxplot(aes(y=Temp))

qg <- q + 
  geom_boxplot(aes(y=Gas))

(qt + qg) +
  patchwork::plot_annotation(title = glue("{cur_dataset} dataset"))
```

Note the two low extremes/outliers for the Gas Consumption after Insulation.  

:::

:::

::: {.callout-note title="Question"}

Build violine plots  of `Temp` and `Gas` versus `Insul`

:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}
```{r}
#| label: violines
#| 
(q + 
  geom_violin(aes(y=Temp))) +
(q + 
  geom_violin(aes(y=Gas))) +
  patchwork::plot_annotation(title = glue("{cur_dataset} dataset"))
```

:::

:::



::: {.callout-note title="Question"}

Plot density estimates of  `Temp` and `Gas` versus `Insul`.

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: densities
r +
  stat_density(alpha=.3 , 
               fill="grey", 
               color="black", 
#               bw = "SJ",
               adjust = .5 ) +
  ggtitle(glue("{cur_dataset} data"))
```

:::

:::

# Hand-made calculation of simple linear regression estimates for `Gas` versus `Temp`

::: {.callout-note title="Question"}

Compute slope and intercept using elementary computations

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: slr-hand-made
#| 
slope <- C[1,2] / C[1,1] # slope 

intercept <- whiteside %$%  # exposing pipe from magrittr
  (mean(Gas) - slope * mean(Temp)) # intercept

# with(whiteside,
#     mean(Gas) - b * mean(Temp)) 
```

In simple linear regression, the slope follows from the covariance matrix 
in a straightforward way. The slope can also be expressed as the linear correlation coefficient (Pearson)  times the ration between the standard deviation of the response variable and the standard deviation of the explanatory variable. 

:::

:::

::: {.callout-note title="Question"}

Overlay the scatterplot with the regression line.

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: overlay-scatterplot
#| 
p + 
  geom_abline(slope=slope, intercept = intercept) +
  ggtitle(glue("{cur_dataset} data"), subtitle = "Least square regression line")
```

:::

:::


# Using `lm()`

`lm` stands for Linear Models. Function `lm` has a number of arguments, including: 

- formula
- data


::: {.callout-note title="Question"}

Use `lm()` to compute slope and intercept. Denote the object created 
by constructor `lm()` as `lm0`. 

- What is the class of `lm0` ?
- 

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: lm0
lm0 <- lm(Gas ~ Temp, data = whiteside)
```

The result is an object of class `lm`. 

The generic function `summary()` has a method for class `lm`

```{r}
#| label: lm0-summary
lm0 %>% 
  summary()
```

The summary is made of four parts 

- The `call`. Very useful if we handle many different models (corresponding to different formulae, or different datasets)
- A numerical summary of *residuals*
- A commented display of the *estimated coefficients*
- Estimate of *noise scale* (under Gaussian assumptions)
- Squared linear correlation coefficient between response variable $Y$ (`Gas`) and predictions $\widehat{Y}$
- A test statistic (Fisher's statistic) for assessing null hypothesis that slope is null, and corresponding $p$-value (under Gaussian assumptions).

:::

:::

Including a rough summary in a report is not always a good idea. It is easy to extract  tabular versions of the summary using functions `tidy()` and `glance()` from package `broom`. 

For html output `gt::gt()` allows us to polish the final output


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

We can use the exposing pipe from `magrittr` (or the `with` construct from base `R`) 
to build a function that extracts the coefficients estimates, standard error, $t$-statistic and associated p-values.  
```{r}
#| label: tidy-steroids
tidy_lm <- . %$% (    # <1>  The lhs is meant to be of class lm
  tidy(.)  %>%         # <2> . acts as a pronoun for magrittr pipes     
  gt::gt() %>% 
  gt::fmt_number(decimals=2) %>% 
  gt::tab_caption(glue("Linear regrression. Dataset: {call$data},  Formula: {deparse(call$formula)}"))  # <3> call is evaluated as a member of the pronoun `.`
)

tidy_lm(lm0)
```

`deparse()` is an important function from base `R`. It is very helpful when trying to take advantage 
of lazy evaluation mechanisms. 



:::

:::

::: {.callout-note title="Question"}

Function `glance()` extract informations that can be helpful when performing model/variable selection. 

:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

The next chunk handles several other parts of the summary. 
```{r}
#| label: glance-steroids
#| 
glance_lm <-  . %$% (
  glance(.) %>% 
  mutate(across(-c(p.value), 
                ~ round(.x, digits=2)),
         p.value=signif(p.value,3)) %>% 
  gt::gt() %>% 
  gt::tab_caption(glue("Dataset: {call$data},  Formula: {deparse(call$formula)}"))
)

glance_lm(lm0)
```

- `r.squared` (and `adj.r.squared`)
- `sigma` estimates the noise standard deviation (under homoschedastic Gaussian noise assumption)
- statistic is the Fisher statistic used to assess the hypothesis that the slope (`Temp` coefficient) is zero. It is compared with quantiles of Fisher distribution with 1 and 54 degrees of freedom (check `pf(47.28, df1=1, df2=54, lower.tail=F)` or `qf(6.55e-09, df1=1, df2=54, lower.tail=F)`).

:::

:::

::: {.callout-note  title="Question"}

`R` offers a function `confint()` that can be fed with objects of class `lm`. Explain the output of this function.

:::

::: {.content-visible when-profile="solution"}

::: {.callout-note title="Solution"}

Under the Gaussian homoschedastic noise assumption, `confint()` 
produces confidence intervals for estimated coefficients. 
Using the union bound, we can derive a conservative confidence rectangle.  

```{r}
#| label: confint-lm
M <- confint(lm0, level=.95)  

as_tibble(M) |>
  mutate(Coeff=rownames(M)) |>
  relocate(Coeff) |>
  gt::gt() |>
  gt::fmt_number(decimals=2)
```

```{r}
#| label: multiple-confidence-rectangle
conf_levels <- c(.90, .95, .99)

purrr::map(conf_levels, \(x) confint(lm0, level=x)) |>
  purrr::map(as_tibble) |>
  purrr::map(\(x) mutate(x, coeffs=c("Intercept", "Temp")))|>
  purrr::map(\(x) pivot_longer(x, cols=ends_with("%"), names_to="conf", values_to="bound")) |>
  bind_rows() |>
  gt::gt() |>
  gt::fmt_number(decimals = 2)


```

:::

:::

::: {.callout-note title="Question"}

Plot a $95\%$ confidence region for the parameters  (assuming homoschedastic Gaussian noise). 


:::


::: {.content-visible when-profile='solution'} 
 
::: {.callout-tip title="Solution"}

Assuming homoschedastic Gaussian noise, the distribution of $(X^T X)^{1/2}\frac{\widehat{\theta}- \theta}{\widehat{\sigma}}$
is $\frac{\mathcal{N}(0, \text{Id}_p)}{\sqrt{\chi^2_{n-p}/(n-p)}}$ with $p=2$ and $n=56$, where  the Gaussian vector and 
the $\chi^2$ variable are independent. Hence
$$\frac{1}{2}\left\Vert (X^T X)^{1/2}\frac{\widehat{\theta}- \theta}{\widehat{\sigma}} \right\Vert^2= \frac{(\widehat{\theta}-\theta)^T X^TX (\widehat{\theta}-\theta)}{2\widehat{\sigma}^2}$$
is distributed accordding to a Fisher distribution with $p=2$ and $n-p=54$ degrees of freedom. Let $f_{\alpha}$ denote the 
quantile of order $1-\alpha$ of the Fisher distribution with $p=2$ and $n-p=54$ degrees of freedom, the following 
ellipsoid is a $1-\alpha$ confidence region:
$$\left\{ \theta : \frac{(\widehat{\theta}-\theta)^T X^TX (\widehat{\theta}-\theta)}{2\widehat{\sigma}^2} \leq f_{\alpha}\right\}$$




```{r}
X <- model.matrix(lm0)
coefficients(lm0)["(Intercept)"]
sum(residuals(lm0)^2)
```




```{r}
#| label: confidence-region
#| 
conf_region <- function(X, x0=0, y0=0, sigma=1, df1=2, df2=nrow(X)-2, alpha=.05, ...){ 

  falpha <- qf(alpha, df1, df2, lower.tail = F)

  specdec <- eigen(t(X) %*% X)
  angle <- acos(specdec$vectors[1,1])
  a <- sigma * sqrt(2*falpha/specdec$values[1])
  b <- sigma * sqrt(2*falpha/specdec$values[2])

  

  ggforce::geom_ellipse(
    aes(
      x0= x0,
      y0= y0,
      a = a,
      b = b,
      angle= angle
    ),
    ...
)
}
```

```{r}
#| label: plot-confidence-region

require(zeallot)  # experimental 

c(x0,y0) %<-% coefficients(lm0)

ggplot() +
  conf_region(X, x0, y0, linetype='dashed') +
  conf_region(X, x0, y0, alpha=.01, linetype='dotted') +
  conf_region(X, x0, y0, alpha=.1, )
  coord_fixed()
```

::: 

:::

# Diagnostic plots

Method `plot.lm()` of generic S3 function `plot`  from base `R` offers six diagnostic plots. By default it displays four of them. 

::: {.callout-tip}

In order to obtain diagnostic plots as ggplot objects, use package `ggfortify` which defines an S3 method for class 'lm' for generic function  `autoplot` (defined 
in package `ggplot`). 

:::

::: {.callout-note title="Question"}

What are the diagnostic plots good for? 

:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

Diagnostic plots for linear regression serve several purposes:

- Visualization of  possible dependencies between residuals and fitted values. OLS theory tells us that residuals and fitted values are (empirically) linearly uncorrelated. 
This does not preclude other kinds of dependencies. 
- With some overplotting, it is also possible to visualize possible dependencies between residuals and variables that were not taken into account while computing OLS estimates. 
- Assesment of possible heteroschedasticity, for example, dependence of noise variance on fitted values.
- Assessing departure of noise distribution from Gaussian setting.
- Spotting points with high leverage
- Spotting possible outliers. 

::: {.callout-note title="Question"}

Load package `ggfortify`, and call `autoplot()` (from `ggplot2`) to build the diagnostic plots for `lm0`. 

Generic function `autoplot()` called on  on an object of class `lm` builds a collection of grapical objects that 
parallel the output of base `R`  generic `plot()` function. The graphical objects output by `autoplot()` can be 
further tweaked as any `ggplot` object.   

:::


::: {.content-visible when-profile='solution'} 
 
 
::: {.callout-tip title="Solution"}



```{r}
#| label: vanilla
vanilla <- c(
  "title"= "Diagnostic plot for linear regression  Gas ~ Temp",
  "caption"= "Whiteside data from package MASS"
)
```


```{r}
#| label: autoplot-lm0
diag_plots <- autoplot(lm0, data=whiteside)
```

`diag_plots` is an instance of an [`S4`  class]() 

```{r}
class(diag_plots)
```

:::

:::


```{r}
diag_plots@plots[[1]] +
  labs(!!!vanilla)
```

This diagnostic plots would be much more helpful if we could map `Insul` on the `shape` aesthetics. 

```{r}
# diag_plots@plots[[1]]  + aes(shape=Insul)
```

```{r}
lm0_augmented <- augment(lm0, data=whiteside) 

lm0_augmented |>
  ggplot() +
  aes(x=.fitted, y=.resid) +
  geom_point(aes(shape=Insul)) +
  geom_smooth(formula='y~x', 
              method="loess", 
              se=T, 
              color="black",
              linewidth=.5) +
  xlab("Fitted values") +
  ylab("Residuals") +
  labs(
    !!!vanilla,
    subtitle = "Residuals versus fitted values"
  )
```

Overplotting (mapping `Insul` over shape aesthetics) reveals that residuals 
do depend on `Insul`: residual signs are a function of `Insul`. 

The *smooth* regression line (blue line) departs from the `x` axis, but not in a striking way.  

```{r}
#| label: patch-diag-plots
patchwork::wrap_plots(
  (diag_plots@plots[[2]] + coord_fixed() + labs()),  
  (
  augment(lm0, whiteside) |>
  ggplot() +
    aes(y=sort(.std.resid), 
        x=qnorm(seq_along(.std.resid)/length(.std.resid))) +
    geom_point(aes(shape=Insul)) + 
    geom_abline(intercept = 0, slope=1.0111398) +
    geom_smooth(method="lm", se=F, linetype="dotted") +
    geom_abline(intercept=0, slope=4/3) +
    xlab("Normal quantiles") +
    ylab("Standardized residuals") +
    coord_fixed() +
    labs()
  )
) +
patchwork::plot_annotation(
  title =vanilla["title"],
  subtitle="QQ plots for linear regression",
  caption = vanilla["caption"]
  )
```

```{r}
diag_plots@plots[[3]] +
  geom_smooth(method="lm", se=T, formula='y ~ x', linetype="dotted") +
  labs(
    !!!vanilla,
    subtitle = "Standardized residuals versus fitted values"
  )
```

The amplitude of *standardized residuals* seems to be an increasing function of 
fitted values. This suggests departure from homoschedasticity. 

Two points are singled out (with row index)


```{r}
diag_plots@plots[[4]] +
  
  labs(
    !!!vanilla,
    subtitle = ""
  )
```
Double check points with high leverage.

Why points with high leverage and large standardized residuals matter. 



The motivation and usage of diagnostic plots is explained in detail in the book by Fox and Weisberg: *An R companion to applied regression*.

In words, the diagnostic plots serve to assess the validity of the Gaussian linear modelling assumptions on the dataset. The assumptions can be challenged for a number of reasons:

- The response variable may be a function of the covariates but not a lienar one. 
- The noise may be heteroschedastic
- The noise may be non-Gaussian
- The noise may exhibit dependencies.  

:::

:::

The diagnostic plots are built from the information gathered in the `lm` object returned by `lm(...)`. 

{{< fa broom >}} It is convenient to extract the required pieces of information using method `augment.lm`. of *generic function* `augment()` from package `broom`. 


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


```{r}
autoplot(lm0)
```


```{r}
#| label: augment-lm0 
whiteside_aug <- lm0 %>% 
  augment(whiteside)

lm0 %$% ( # exposing pipe !!! 
  augment(., data=whiteside) %>% 
  mutate(across(!where(is.factor), ~ signif(.x, 3))) %>% 
  group_by(Insul) %>% 
  sample_n(5) %>% 
  ungroup() %>% 
  gt::gt() %>% 
  gt::tab_caption(glue("Dataset {call$data},  {deparse(call$formula)}"))
)
```

:::

:::

Recall that in the output of `augment()`

- `.fitted`: $\widehat{Y} = H \times Y= X \times \widehat{\beta}$ 
- `.resid`: $\widehat{\epsilon} = Y - \widehat{Y}$ residuals, $\sim (\text{Id}_n - H) \times \epsilon$ 
- `.hat`: diagonal coefficients of Hat matrix $H$ 
- `.sigma`: is meant to be the estimated standard deviation of components of  $\widehat{Y}$

Compute the share of *explained variance* 

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
whiteside_aug %$% {
  1 - (var(.resid)/(var(Gas)))
}

# with(whiteside_aug,
#   1 - (var(.resid)/(var(Gas)))
```

:::

:::

Plot residuals against fitted values 

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}

diag_1 <- whiteside_aug %>% 
  ggplot() +
  aes(x=.fitted, y=.resid)+
  geom_point(aes(shape= Insul), size=1, color="black") +
  geom_smooth(formula = y ~ x,
              method="loess",
              se=F,
              linetype="dotted",
              linewidth=.5,
              color="black") +
  geom_hline(yintercept = 0, linetype="dashed") +
  xlab("Fitted values") +
  ylab("Residuals)") +
  labs(caption = "Residuals versus Fitted")
```

:::

:::

::: {.callout-note title="Question"}

Fitted against square root of standardized residuals.

:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
diag_3 <- whiteside_aug %>%
  ggplot() +
  aes(x=.fitted, y=sqrt(abs(.std.resid))) +
  geom_smooth(formula = y ~ x,
              se=F,
              method="loess",
              linetype="dotted",
              linewidth=.5,
              color="black") +
  xlab("Fitted values") +
  ylab("sqrt(|standardized residuals|)") +
  geom_point(aes(shape=Insul), size=1, alpha=1) +
  labs(caption = "Scale location")
```

:::

:::

::: {.callout-note title="Question"}

Hand-made normal qqplot for `lm0` 

:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


```{r}
diag_2 <- whiteside_aug %>% 
  ggplot() +
  aes(sample=.std.resid) +
  geom_qq(size=.5, alpha=.5) +
  stat_qq_line(linetype="dotted",
              linewidth=.5,
              color="black") +
#  coord_fixed() +
  labs(caption="Residuals qqplot") +
  xlab("Theoretical quantiles") +
  ylab("Empirical quantiles of standadrdized residuals")
```

:::

:::

::: {.callout-note title="Question"}



:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
(diag_1 + diag_2 + diag_3 + guide_area()) + 
  plot_layout(guides="collect") +
  plot_annotation(title=glue("{cur_dataset} dataset"),
                  subtitle = glue("Regression diagnostic  {deparse(lm0$call$formula)}"), caption = 'The fact that the sign of residuals depend on Insul shows that our modeling is too naive.\n The qqplot suggests that the residuals are not collected from Gaussian homoschedastic noise.'
                  )
```

:::

:::


# Taking into account Insulation

::: {.callout-note title="Question"}

Design a *formula* that allows us to take into account the possible 
impact of Insulation. Insulation may impact the relation between weekly `Gas` consumption and average external `Temperature` in two ways. Insulation may modify the `Intercept`, it may also modify the slope, that is the sensitivity of `Gas` consumption  with respect to average external `Temperature`. 

:::


::: {.callout-tip}

Have a look at formula documentation (`?formula`).

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


```{r}
#| label: big-formula
lm1 <- lm(Gas ~ Temp * Insul, data = whiteside)

vanilla_lm1 <- vanilla
vanilla_lm1["title"] <- "Diagnostic plot for linear regression  Gas ~ Temp*Insul"
```
:::

:::

::: {.callout-note title="Question"}

Check the design using function `model.matrix()`. How can you relate this augmented design and the *one-hot encoding* of variable `Insul`?


:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: check-design
model.matrix(lm1) |>
  head()
```

`InsulAfter` is the result of *one-hot encoding* of boolean column `Insul`: 
`Before` is encoded by `0` and `After` is encoded by `1`. 

`Temp:InsulAfter` is the elementwise product of `Temp`  and `InsulAfter`. 

In *one-hot-encoding*, a categorical columns with `k` levels is mapped to `k` `0-1` valued 
columns. An occurrence of level `j : 1 ≤ j ≤ k` is mapped to a sequence of `j-1` `0`'s, a `1`, followed by `k-j` `0`'s.   
The resulting matrix has column rank at most 'k-1', since the rowsums are all equal to `1`. 
In order to eliminate this redundancy, one of the `k` columns is dropped. 
Here the column with `1`'s for `Before`  has been dropped. 

Note that other encodings (called *contrasts* in the statistics literature) are possible.

 
:::

:::

::: {.callout-note title="Question"}

Display and comment the part of the summary of `lm1` concerning estimated coefficients.

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: lm1-tidy
lm1 %>% 
  tidy_lm()
```

:::

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: lm0-lm1-glance
bind_rows(
  lm0 |>
    glance() |>
    mutate(model="Gas ~ Temp"),
  lm1 |>
    glance() |>
    mutate(model="Gas ~ Temp*Insul")
) |>
  relocate(model) |>
  gt::gt() |>
  gt::fmt_number(decimals=2) |>
  gt::tab_caption("Comparing two models for Whiteside dataset")
```

:::

:::

::: {.callout-note title="Question"}

Comment the diagnostic plots built from the extended model using `autoplot()`. If possible, 
generate alternative diagnostic plots pipelining `broom`  and `ggplot2`. 

:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


```{r}
#| label: autoplot-lm1
diag_plots_lm1 <- autoplot(lm1, data=whiteside)
```

```{r}
#| label: lm1-residuals-fitted
diag_plots_lm1@plots[[1]] +
  labs(
    !!!vanilla_lm1,
    subtitle="Residuals versus fitted values"
  )
```

```{r}
#| label: lm1-residuals-fitted-hand-made

whiteside_aug1 <-  augment(lm1, whiteside)

whiteside_aug1 |>
  ggplot() + 
  aes(x=.fitted, y=.resid) +
  geom_point(aes(shape=Insul)) +
  geom_smooth(method="loess", formula='y ~ x') +
  labs(
    !!!vanilla_lm1,
    subtitle="Residuals versus fitted values"
  )
```

The sign of residuals does  not look like a function of `Insul` anymore. 

```{r}
#| label: tweaking-p
#| 
p +
  geom_smooth(formula='y ~ poly(x, 2)',linewidth=.5, color="black",linetype="dashed",  method="lm", se=FALSE)+
  aes(color=Insul) +
  geom_smooth(aes(linetype=Insul), 
              formula='y ~ x',linewidth=.5, color="black", method="lm", se=FALSE) +
  scale_color_manual(values= c("Before"="red", "After"="blue")) +
  geom_abline(intercept = 6.8538, slope=-.3932, color="red") +
  geom_abline(intercept = 6.8538 - 2.13, slope=-.3932 +.1153, color="blue") + labs(
    title=glue("{cur_dataset} dataset"),
    subtitle = glue("Regression: {deparse(lm1$call$formula)}")
    )
```

:::

:::

::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r lm1-diagnostics}
#| label: diag-plots-lm1 

(diag_1 %+% whiteside_aug1) +
(diag_3 %+% whiteside_aug1) +  
(diag_2 %+% whiteside_aug1) +
  guide_area() +
  plot_layout(guides = "collect",) +
  plot_annotation(title=glue("{cur_dataset} dataset"),
                  subtitle = glue("Regression diagnostic  {deparse(lm1$call$formula)}"), caption = 'One possible outlier.\n Visible on all three plots.'
                  )
```


The formula argument defines the design matrix and the Least-Squares problem used to estimate the coefficients.

```{r}
cowplot::plot_grid(
  (diag_1 %+% whiteside_aug1),
  (diag_3 %+% whiteside_aug1),
  (diag_2 %+% whiteside_aug1),
  align="none",
  nrow=3, 
  labels=c('A', 'B', 'C')
)
```

```{r}
diag_plots_lm1@plots[[1]] + 
  diag_plots_lm1@plots[[3]] +
  patchwork::plot_annotation(
    title="Gas ~ Temp * Insul",
    # subtitle = "Residuals versus fitted",
    caption="Whiteside data"
  )
```

```{r}
diag_plots_lm1@plots[[2]] +
  labs(!!!vanilla_lm1)
```



```{r}
diag_plots@plots[[4]] +
  labs(
    !!! vanilla_lm1
  )
```

:::

:::


Function `model.matrix()` allows us to inspect the design matrix. 


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r design_matrix}
#| label: lm1-design-matrix
model.matrix(lm1) %>% 
  as_tibble() %>% 
  mutate(Insul=ifelse(InsulAfter,"After", "Before")) %>% 
  ungroup() %>% 
  slice_sample(n=5) |>
  gt::gt() |>
  gt::tab_caption(glue("Design matrix for {deparse(lm1$call$formula)}")) 
```

```{r design-matrix}
#| label: extract-design-matrix-lm1
#| 
X <- model.matrix(lm1)
```

:::

:::


In order to solve le Least-Square problems, we have to compute 
$$(X^T \times X)^{-1} \times X^T$$
This can be done in several ways.

`lm()` uses QR factorization. 


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


```{r }
#| label: QR-design-matrix
#| 
Q <- qr.Q(lm1$qr)
R <- qr.R(lm1$qr)  # R is upper triangular 

norm(X - Q %*% R, type="F") # QR Factorization

signif(t(Q) %*% Q, 2)      # Q's columns form an orthonormal family

H <- Q %*% t(Q)             # The Hat matrix 

norm(X - H %*% X, type="F") # H leaves X's columns invariant
norm(H - H %*% H, type="F") # H is idempotent



# eigen(H, symmetric = TRUE, only.values = TRUE)$values
```

```{r}
#| label: solving-normal-equations
sum((solve(t(X) %*% X) %*% t(X) %*% whiteside$Gas - lm1$coefficients)^2)
```

Once we have the QR factorization of $X$, solving the normal equations boils down to inverting a triangular matrix. 

```{r}
#| label: solving-normal-equations-r
sum((solve(R) %*% t(Q) %*% whiteside$Gas - lm1$coefficients)^2)
```

:::

:::

```{r}
#matador::mat2latex(signif(solve(t(X) %*% X), 2))
```

$$
(X^T \times X)^{-1} = \begin{bmatrix}
0.18 & -0.026 & -0.18 & 0.026\\
-0.026 & 0.0048 & 0.026 & -0.0048\\
-0.18 & 0.026 & 0.31 & -0.048\\
0.026 & -0.0048 & -0.048 & 0.0099
\end{bmatrix}
$$


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


```{r }
#| label: fitted-H-matrix
whiteside_aug1 %>% 
  glimpse()
```

:::

:::


Understanding `.fitted` column


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: predict-project
sum((predict(lm1, newdata = whiteside) - whiteside_aug1$.fitted)^2)

sum((H %*% whiteside_aug1$Gas - whiteside_aug1$.fitted)^2)
```

:::

:::

::: {.callout-note title="Question"}

Try understanding the computation of `.resid` in an `lm` object. 
Compare `.resid` with the projection of `Gas` on the linear subspace orthogonal 
to the columns of the design matrix. 


:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: check-signif-resid
sum((whiteside_aug1$.resid + H %*% whiteside_aug1$Gas - whiteside_aug1$Gas)^2)
```

:::

:::

::: {.callout-note title="Question"}

Understanding `.hat`


:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#|  label: interpret-hat-column-hat-matrix
sum((whiteside_aug1$.hat - diag(H))^2)
```

:::

:::


::: {.callout-note title="Question"}

Understanding `.std.resid` 

- Estimate noise intensity from `residuals`
- Compare with the output of `glance()`


:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}

```{r}
#| label: check-sigma 
sigma_hat <- sqrt(sum(lm1$residuals^2)/lm1$df.residual)

lm1 |>
  glance() |>
  gt::gt() |>
  gt::fmt_number(decimals=2) |>
  gt::tab_caption(glue("The hand-made estimate of sigma is {round(sigma_hat,2)}"))
```

$$
\widehat{r}_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma} \sqrt{1 - H_{i,i}}}
$$


```{r}
#| label: check-standardized-residuals
sum((sigma_hat * sqrt(1 -whiteside_aug1$.hat) * whiteside_aug1$.std.resid - whiteside_aug1$.resid)^2)
```


:::

:::

::: {.callout-note title="Question"}

Understanding column `.sigma` 

:::


::: {.content-visible when-profile="solution"}

::: {.callout-tip title="Solution"}


Column `.sigma` contains the *leave-one-out* estimates of $\sigma$, that is `whiteside_aug1$.sigma[i]` is the estimate of $\sigma$ you obtain by leaving out the `i` row of the dataframe. 

There is no need to recompute everything for each sample element.

$$
\widehat{\sigma}^2_{(i)} =  \widehat{\sigma}^2 \frac{n-p-1- \frac{\widehat{\epsilon}_i^2}{\widehat{\sigma}^2 {(1 - H_{i,i})}}\frac{}{}}{n-p-2}
$$

:::

:::


# Appendix

## `S3`  classes in `R`




## Generic functions for `S3`  classes

`methods(autoplot)` lists the `S3` classes for which an  autoplot method is defined. 
Some methods are defined in `ggplot2`, others like `autoplot.lm` are defined in extension 
packages like `ggfortify`. 



## `S4` classes in `R`

The output of `autoplot.lm` is an instance  of `S4` class


## `tibbles` with list columns


